# Code-Reproduction-for-Audio-Visual-Representation-system

I learned from the paper published in CVPR 2021 named 'Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation' (github link: https://hangznju-cuhk.github.io/projects/PC-AVS) and made a few changed to the codes to make it applicable in my own input data. 

For example, the screenshot for gesture input is: 
![image](https://github.com/DanyXuXX/Code-Reproduction-for-Audio-Visual-Representation-system/assets/77055103/28b0bdb5-4c90-4a2f-9b84-38dc9c1cc9ff)

The screenshot for mouth pose and voice input is: 
![image](https://github.com/DanyXuXX/Code-Reproduction-for-Audio-Visual-Representation-system/assets/77055103/a99d897e-4546-4ce4-b49a-334fbd449bc6)


The screenshot for applied target input is: 
![image](https://github.com/DanyXuXX/Code-Reproduction-for-Audio-Visual-Representation-system/assets/77055103/2346d580-769c-4374-a4f4-e7da8d9c78fa)


The screenshot for talking face generated output is: 
![image](https://github.com/DanyXuXX/Code-Reproduction-for-Audio-Visual-Representation-system/assets/77055103/40c8585f-1db9-4844-91ec-98f192ec7836)


The video example is stored in file 'example'.
